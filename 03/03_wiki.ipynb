{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "# 한국어 위키백과\r\n",
    "# 3강\r\n",
    "# 참고: https://omicro03.medium.com/%EC%9E%90%EC%97%B0%EC%96%B4%EC%B2%98%EB%A6%AC-nlp-14%EC%9D%BC%EC%B0%A8-word2vec-%EC%8B%A4%EC%8A%B5-a4e7767a1e66\r\n",
    "\r\n",
    "# 1. 위키피디아 한국어 덤프 파일 다운로드\r\n",
    "# 다운로드 : https://dumps.wikimedia.org/kowiki/latest/\r\n",
    "# 2. 위키피디아 익스트랙터 다운로드 : 위키 범프에서 위키 문서의 제목과 본문만 추출한다.\r\n",
    "# https://github.com/attardi/wikiextractor\r\n",
    "# 3. 위키피디아 한국어 덤프 파일 변환\r\n",
    "# 4. 훈련 데이터 만들기\r\n",
    "# 5. Word2Vec 작업"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "# ----------------------------------------------------------------\r\n",
    "# 라이브러리\r\n",
    "from gensim.corpora import WikiCorpus, Dictionary\r\n",
    "from gensim.utils import to_unicode\r\n",
    "import re"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\lemon\\anaconda3\\envs\\pnlp\\lib\\site-packages\\gensim\\similarities\\__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# ----------------------------------------------------------------\r\n",
    "# 전처리할 덤프 파일 경로\r\n",
    "in_f = 'D:/pseudo_data/raw/kowiki-latest-pages-articles.xml.bz2'\r\n",
    "# 저장할 텍스트 경로\r\n",
    "out_f = 'D:/pseudo_data/processed/processed_wiki_ko.txt'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# -------------------------------------------------------------\r\n",
    "# 말뭉치 만드는 함수 정의\r\n",
    "def make_corpus(in_f, out_f):\r\n",
    "    \"\"\"위키피디아 xml 덤프를 test 형식으로 변환\"\"\"\r\n",
    "    output = open(out_f, 'w', encoding = 'utf-8')\r\n",
    "    wiki = WikiCorpus(in_f, tokenizer_func=tokenize, dictionary=Dictionary())\r\n",
    "    i = 0\r\n",
    "    for text in wiki.get_texts():\r\n",
    "        output.write(bytes(' '.join(text), 'utf-8'.decode('utf-8') + '\\n'))\r\n",
    "        i = i + 1\r\n",
    "        if (i % 10000 == 0) :\r\n",
    "            print('Processed' + str(i) + 'articles')\r\n",
    "    output.close()\r\n",
    "    print('Processing complete!')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# ----------------------------------------------------------------\r\n",
    "# 사용자 정의 tokenize 정의\r\n",
    "# 특수문자, 공백, 이메일주소, 웹 페이지 주소 등을 제거한다.\r\n",
    "\r\n",
    "WIKI_REMOVE_CHARS = re.compile(\"'+|.{2,30}=+)|__TOC__|(ファイル:).+|:(en|de|it|fr|es|kr|zh|no|fi):|\\n\", re.UNICODE)\r\n",
    "# re.compile\r\n",
    "# \"'+|.{2,30}=+)|__TOC__|(ファイル:).+|:(en|de|it|fr|es|kr|zh|no|fi):|\\n\"\r\n",
    "# e.UNICODE\r\n",
    "WIKI_SPACE_CHARS = re.compile(\"(\\\\s|゙|゚|　)+\", re.UNICODE)\r\n",
    "EMAIL_PATTERN = re.compile(\"(^[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+$)\", re.UNICODE)\r\n",
    "URL_PATTERN = re.compile(\"(ftp|http|https)?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\", re.re.UNICODE)\r\n",
    "WIKI_REMOVE_TOKEN_CHARS = re.compile(\"(\\\\*$|:$|^파일:.+|^;)\", re.UNICODE)\r\n",
    "MULTIPLE_SPACE = re.compile(' +', re.re.UNICODE)\r\n",
    "\r\n",
    "def tokenize(content, token_min_len = 2, token_max_len=100, lower=True):\r\n",
    "    content = re.sub(EMAIL_PATTERN, ' ', cotent)\r\n",
    "    content = re.sub(URL_PATTERN, ' ', cotent)\r\n",
    "    content = re.sub(WIKI_REMOVE_CHARS, ' ', cotent)\r\n",
    "    content = re.sub(WIKI_SPACE_CHARS, ' ', cotent)\r\n",
    "    content = re.sub(MULTIPLE_SPACE, ' ', cotent)\r\n",
    "    tokens = content.replace(\", )\", \"\").split(\" \")\r\n",
    "    result = []\r\n",
    "    for token in tokens:\r\n",
    "        if not token.startswith('_'):\r\n",
    "            token_candidate = to_unicode(re.sub(WIKI_REMOVE_TOKEN_CHARS, '', token))\r\n",
    "        else :\r\n",
    "            token_candidate = \"\"\r\n",
    "        if len(token_candidate) > 0 :\r\n",
    "            result.append(token_candidate)\r\n",
    "    return result"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.6.5",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.5 64-bit ('pnlp': conda)"
  },
  "interpreter": {
   "hash": "034986b1388e023bda407c3a8ff79801d799a089d00b4b61f6ebe20fc1b4aa17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}